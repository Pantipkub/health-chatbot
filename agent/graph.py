from dotenv import load_dotenv  
from langchain_core.messages import BaseMessage # The foundational class for all message types in LangGraph
from langchain_core.messages import ToolMessage # Passes data back to LLM after it calls a tool such as the content and the tool_call_id
from langchain_core.messages import SystemMessage # Message for providing instructions to the LLM
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import tool
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from .state import AgentState
from .rag_utils import retrieve_context

load_dotenv()

@tool
def add(a: int, b:int):
    """This is an addition function that adds 2 numbers together"""

    return a + b 

@tool
def subtract(a: int, b: int):
    """Subtraction function"""
    return a - b

@tool
def multiply(a: int, b: int):
    """Multiplication function"""
    return a * b

# tools = [add, subtract, multiply]
tools = []

# model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite").bind_tools(tools)

intent_model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0
)

chat_model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash"
)


# ----- Function -----

def input_node(state: AgentState):
    """
    Entry node that receives user input and initializes debugging metadata.
    """
    
    state["current_node"] = "input_node"
    state["steps"].append("Received user input")

    last_msg = state["messages"][-1]
    print("User said:", last_msg.content)

    return state

def classify_intent_node(state: AgentState):
    """
    Analyze user message and classify medical intent using LLM.
    """

    state["current_node"] = "classify_intent"
    state["steps"].append("Classifying user intent with LLM")

    prompt = SystemMessage(content=
        "You are a medical triage assistant.\n"
        "Classify the user's intent into ONE of the following:\n"
        "- symptom\n"
        "- general_health\n"
        "- administrative\n\n"
        "Respond with only the label."
    )

    user_msg = state["messages"][-1]
    result = intent_model.invoke([prompt, user_msg])

    state["intent"] = result.content.strip().lower()
    return state

def route_by_intent(state):
    intent = state.get("intent")
    if intent in ("symptom", "general_health"):
        return intent
    return "general_health"  # fallback

# ----- Conditional -----
def should_continue(state: AgentState):
    """
    Decide whether to continue calling the tools.
    """

    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls: # ‡πÑ‡∏°‡πà‡∏°‡∏µ tool_calls ‡∏à‡∏≤‡∏Å LLM ‡πÅ‡∏•‡πâ‡∏ß = ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡∏ï‡∏≠‡∏ö User ‡πÅ‡∏•‡πâ‡∏ß

        return "end"
    else:
        return "continue"   # LLM ‡∏Ç‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ tool ‡∏≠‡∏¢‡∏π‡πà
    
def call_model(state: AgentState):
    """
    Node ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î (Thinking Process) ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ
    """
    messages = state["messages"]
    last_user_message = messages[-1].content 
    context = retrieve_context(last_user_message)
    
    if context:
        rag_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏π‡πÅ‡∏•‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏±‡πâ‡∏ô‡πÜ\n"
            "**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å**: ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n\n"
            "### üí≠ ‡∏Ñ‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà\n"
            "(‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£ ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÑ‡∏´‡∏ô‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏ô‡∏ö‡πâ‡∏≤‡∏á)\n\n"
            "### üìã ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥\n"
            "(‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡∏ö‡∏≠‡∏Å‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥ 3 ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)\n\n"
            f"--- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ---\n{context}\n"
            "--------------------\n\n"
            "‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:\n"
            "1. ‡∏™‡πà‡∏ß‡∏ô '‡∏Ñ‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà' ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏≥‡∏û‡∏∂‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n"
            "2. ‡∏™‡πà‡∏ß‡∏ô '‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•' ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏´‡πâ‡∏≤‡∏°‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÇ‡∏£‡∏Ñ‡∏ü‡∏±‡∏ô‡∏ò‡∏á\n"
            "3. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á"
        )
    else:
        rag_prompt = (
            "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ú‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á '‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô ‡πÇ‡∏£‡∏Ñ‡πÑ‡∏ï ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô‡πÇ‡∏•‡∏´‡∏¥‡∏ï ‡πÅ‡∏•‡∏∞‡πÑ‡∏Ç‡∏°‡∏±‡∏ô‡πÉ‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏î' ‡∏Ñ‡∏£‡∏±‡∏ö "
            "‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏°‡∏°‡∏µ "
            "‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏ú‡∏°‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö"
        )

    response = chat_model.invoke([SystemMessage(content=rag_prompt)] + messages)
    
    return {"messages": [response], "steps": ["retrieval", "generate"]}

# ----- Generate graph -----
def build_graph():
    graph = StateGraph(AgentState)

    # ----- add nodes -----
    graph.add_node("input", input_node)
    graph.add_node("classify_intent", classify_intent_node)
    graph.add_node("our_agent", call_model)

    tool_node = ToolNode(tools=tools)
    graph.add_node("tools", tool_node)

    # ----- entry point -----
    graph.set_entry_point("input")

    # ----- normal edges -----
    graph.add_edge("input", "classify_intent")

    # ----- routing by intent -----
    graph.add_conditional_edges(
        "classify_intent",
        route_by_intent,
        {
            "symptom": "our_agent",
            "general_health": "our_agent"
        }
    )

    # ----- tool loop (optional ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ tools = []) -----
    graph.add_conditional_edges(
        "our_agent",
        should_continue,
        {
            "continue": "tools",
            "end": END
        }
    )

    graph.add_edge("tools", "our_agent")

    app = graph.compile()

    return app
